{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893c624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial\n",
    "\n",
    "from math import sqrt\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mordred import Calculator, descriptors\n",
    "#import openbabel\n",
    "from openbabel import pybel\n",
    "from PyBioMed.PyMolecule.fingerprint import CalculatePubChemFingerprint,CalculateECFP2Fingerprint\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import Atom\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_curve, auc \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as G_Loader \n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "\n",
    "# RDkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "# Pytorch and Pytorch Geometric\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # activation function\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader as V_Loader # dataset management\n",
    "%run ./graph_feature.ipynb \n",
    "%run ./dataset_processing.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b141c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "final_clean_fingerp_train=[]\n",
    "final_clean_fingerp_val=[]\n",
    "for i in range(k):\n",
    "    final_clean_fingerp_train.append(np.load('final_clean_fingerp_train'+ str(i)+'.npy'))\n",
    "    final_clean_fingerp_val.append(np.load('final_clean_fingerp_val' +str(i)+'.npy'))\n",
    "\n",
    "final_clean_fingerp_test = np.load('final_clean_fingerp_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d82b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.load('train_indices.npy')\n",
    "val_idx = np.load('val_indices.npy')\n",
    "test_idx = np.load('test_indices.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the output label \n",
    "total_train_targets =[]\n",
    "total_validation_targets =[]\n",
    "total_test_targets=[]\n",
    "for i in range(k):\n",
    "    total_train_targets.append(np.load('total_train_targets'+ str(i)+'.npy'))\n",
    "    total_validation_targets.append(np.load('total_validation_targets' +str(i)+'.npy'))\n",
    "\n",
    "total_test_targets= np.load('total_test_targets.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for training (vector data)\n",
    "#======================================================================================\n",
    "list_data_fingerp_train =[]\n",
    "list_data_fingerp_val =[]\n",
    "list_data_target_train =[]\n",
    "list_data_target_val =[]\n",
    "\n",
    "for data_train, data_val, tr_targets, val_targets in zip(final_clean_fingerp_train, final_clean_fingerp_val,total_train_targets, total_validation_targets):\n",
    "    train_loader = V_Loader(dataset = data_train, batch_size = 500)\n",
    "    val_loader = V_Loader(dataset = data_val, batch_size = 500)\n",
    "    \n",
    "    tr_target_loader = V_Loader(dataset = tr_targets, batch_size = 500)\n",
    "    val_target_loader =  V_Loader(dataset = val_targets, batch_size = 500)\n",
    "    \n",
    "    list_data_fingerp_train.append(train_loader)\n",
    "    list_data_fingerp_val.append(val_loader)\n",
    "    \n",
    "    list_data_target_train.append(tr_target_loader)\n",
    "    list_data_target_val.append(val_target_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#define the loss function \n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac3d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1(train_v_loaderB,tr_target_v_loaderB, combined_model, optimizer):\n",
    "    combined_model.train()\n",
    "    \n",
    "    for data_X2, data_target in zip(train_v_loaderB, tr_target_v_loaderB):  # Iterate in batches over the training dataset.\n",
    "        out = combined_model(torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        \n",
    "        y_t = torch.tensor(data_target, dtype=torch.float32)\n",
    "        loss = criterion(out[:,0], y_t)  # Compute the loss.\n",
    "        #print(k)\n",
    "        #print(loss)\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "    return loss, combined_model, optimizer\n",
    "\n",
    "def validation_1(val_v_loaderB,val_target_v_loaderB, combined_model):\n",
    "    for data_X2, data_target in zip(val_v_loaderB, val_target_v_loaderB): # Iterate in batches over the training dataset.\n",
    "        out = combined_model(torch.tensor(data_X2, dtype=torch.float32))  # Perform a single forward pass.\n",
    "        \n",
    "        y_t = torch.tensor(data_target, dtype=torch.float32)\n",
    "        val_loss = criterion(out[:,0], y_t)  # Compute the loss.\n",
    "        \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c1d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, roc_curve, auc \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# performances visualization \n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import statistics\n",
    "from prettytable import PrettyTable\n",
    "%run ./my_performances.ipynb \n",
    "\n",
    "\n",
    "def test_1(v_loaderB,v_target, combined_model):\n",
    "    combined_model.eval()\n",
    "    list_pred =[]\n",
    "    list_targets =[]\n",
    "    correct = 0\n",
    "    for data_X2, data_target in zip (v_loaderB,v_target):  # Iterate in batches over the training/test dataset.\n",
    "            out = combined_model(torch.tensor(data_X2, dtype=torch.float32))\n",
    "            out_1 = out[:,0]\n",
    "            \n",
    "            \n",
    "            list_pred.append(out_1.item())\n",
    "            list_targets.append(data_target.item())\n",
    "    return list_pred, list_targets \n",
    "\n",
    "# used to count the train accuracy ,and validation accuracy when in the training mode \n",
    "def test(v_loaderB,target_v_loaderB, combined_model):\n",
    "    combined_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data_X2, data_target in zip(v_loaderB,target_v_loaderB):  # Iterate in batches over the training/test dataset.\n",
    "            out = combined_model(torch.tensor(data_X2, dtype=torch.float32))\n",
    "            out_1 = out[:,0]\n",
    "            for i,value in enumerate(out_1) :\n",
    "                if value > 0.5 :\n",
    "                    out_1[i] = 1\n",
    "                else : out_1[i] = 0\n",
    "            pred = out_1  # Use the class with highest probability.\n",
    "            correct += int((pred == data_target).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(v_loaderB.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df32bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(gnn_model, learning_rate, optimizer_type):\n",
    "    if optimizer_type==1:\n",
    "        optimizer = torch.optim.SGD(gnn_model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    if optimizer_type==2:\n",
    "        optimizer = torch.optim.Adam(gnn_model.parameters(), lr=learning_rate, weight_decay =1e-4)\n",
    "    if optimizer_type ==3 :\n",
    "        optimizer = torch.optim.Adamax(gnn_model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc04267",
   "metadata": {},
   "outputs": [],
   "source": [
    " class modelB1(torch.nn.Module):\n",
    "    def __init__(self, input_features, output_features,dropout_rateB1,dropout_rateB2, dense_layer1):\n",
    "        super(modelB1, self).__init__()\n",
    "        self.lin1 = nn.Linear(input_features,dense_layer1)\n",
    "      \n",
    "        self.lin2 = nn.Linear(int(dense_layer1), int(dense_layer1/2))\n",
    "        self.lin3 = nn.Linear(int(dense_layer1/2), int(dense_layer1/2))\n",
    "        self.lin4 = nn.Linear(int(dense_layer1/2), output_features)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(int(dense_layer1))\n",
    "        self.bn2 = nn.BatchNorm1d(int(dense_layer1/2))\n",
    "        self.bn3 = nn.BatchNorm1d(int(dense_layer1/2))\n",
    "        self.dropoutB1 = dropout_rateB1\n",
    "        self.dropoutB2 = dropout_rateB2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.bn1(x)\n",
    "       \n",
    "        x = F.dropout(x, p= self.dropoutB1, training=self.training)\n",
    "        x = x.relu()\n",
    "  #      \n",
    "        x = self.lin2(x)\n",
    "        x = self.bn2(x)   \n",
    "        x = F.dropout(x, p= self.dropoutB2, training=self.training)\n",
    "        x = x.relu()\n",
    "  #      \n",
    "        x = self.lin3(x)\n",
    "        x = self.bn3(x)   \n",
    "        x = x.relu()\n",
    "        x = self.lin4(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_data_vec_trainB = list_data_fingerp_train\n",
    "list_data_vec_valB = list_data_fingerp_val\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    k=10 # number of fold \n",
    "    \n",
    "    # Categorical parameter\n",
    "    # Create k model for training k fold dataset \n",
    "    # for data graph\n",
    "    input_featuresB    = 36 # length of feature data vector \n",
    "    output_featuresB   = 1\n",
    "      \n",
    "    optimizer_type = trial.suggest_int('optimizer_type',1,3, step=1) \n",
    "    #optimizer_type = 2\n",
    "    #dense_layer1 = 58\n",
    "    #dropout_rateB1 = 0.11271207164779487\n",
    "    #dropout_rateB2 = 0.24808286943977564\n",
    "    dense_layer1 =trial.suggest_int('dense_layer1',32,128, step=2) \n",
    "    #dense_layer1= 64\n",
    "    dropout_rateB1 = trial.suggest_uniform('dropout_rateB1',  0.1, 0.5)\n",
    "    dropout_rateB2 = trial.suggest_uniform('dropout_rateB2',  0.1, 0.5)\n",
    "    \n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 0.0001, 0.001)\n",
    "    #learning_rate = 0.0008197186615142072\n",
    "    \n",
    "    \n",
    "    list_trained_model =[]\n",
    "    list_val_acc=[]\n",
    "    \n",
    "    for i in range(k):\n",
    "        combined_model   = modelB1(input_featuresB, output_featuresB,dropout_rateB1,dropout_rateB1, dense_layer1)\n",
    "        optimizer = get_optimizer(combined_model, learning_rate, optimizer_type)\n",
    "   \n",
    "        history_train_loss=[]\n",
    "        history_val_loss=[]\n",
    "        history_train_acc=[]\n",
    "        history_val_acc=[]\n",
    "        \n",
    "        for epoch in range(1,500):\n",
    "            \n",
    "                \n",
    "            train_loss,combined_model, optimizer = train_1(list_data_vec_trainB[i],list_data_target_train[i],\n",
    "                                                           combined_model,optimizer)\n",
    "            val_loss                             = validation_1(list_data_vec_valB[i],list_data_target_val[i],\n",
    "                                                                combined_model) \n",
    "        \n",
    "            train_acc                            = test(list_data_vec_trainB[i],list_data_target_train[i],combined_model)\n",
    "            val_acc                              = test(list_data_vec_valB[i],list_data_target_val[i], combined_model)\n",
    "        \n",
    "           \n",
    "            \n",
    "            history_train_loss.append(train_loss.item())\n",
    "            history_val_loss.append(val_loss.item())\n",
    "            history_train_acc.append(train_acc)\n",
    "            history_val_acc.append(val_acc)\n",
    "        \n",
    "            print(f'Fold: {i}')\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss:, {train_loss.item():.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, val Loss:, {val_loss.item():.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, val Acc: {val_acc:.4f}')\n",
    "            print(\" \")\n",
    "            # RUNNING THE 5 PRETRAINED MODEL USING INDEPENDENT DATA\n",
    "        #list_val_acc.append(val_acc)\n",
    "        list_trained_model.append(combined_model)\n",
    "    return statistics.mean(list_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyper = {'optimizer_type': 3, 'dense_layer1': 116, 'dropout_rateB1': 0.19140697875883203, 'dropout_rateB2': 0.4378593600513469, 'learning_rate': 0.0005085512117234079}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c8c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10 # number of fold \n",
    "\n",
    "# for data vector \n",
    "input_features  = 36 # length of feature data vector \n",
    "output_features   = 1\n",
    "    #output_featuresB   = trial.suggest_int('output_featuresB',16,9, step=1) \n",
    "optimizer_type =my_hyper['optimizer_type']   \n",
    "dropout_rateB1 = my_hyper['dropout_rateB1']\n",
    "dropout_rateB2 = my_hyper['dropout_rateB2']\n",
    "learning_rate = my_hyper['learning_rate']\n",
    "dense_layer1 =my_hyper['dense_layer1']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632124fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY THE HYPERPARAMETER \n",
    "# RUNNING EVERY FOLD \n",
    "#=============================================================\n",
    "\n",
    "total_history_train_loss=[]\n",
    "total_history_val_loss=[]\n",
    "total_history_train_acc=[]\n",
    "total_history_val_acc=[]\n",
    "list_trained_model =[]\n",
    "# APPLY THE HYPERPARAMETER \n",
    "# RUNNING EVERY FOLD \n",
    "#=============================================================\n",
    "\n",
    "total_history_train_loss=[]\n",
    "total_history_val_loss=[]\n",
    "total_history_train_acc=[]\n",
    "total_history_val_acc=[]\n",
    "list_trained_model =[]\n",
    "\n",
    "for i in range(k):\n",
    "        \n",
    "        combined_model   = modelB1(input_features, output_features,dropout_rateB1,dropout_rateB2, dense_layer1)\n",
    "        optimizer = get_optimizer(combined_model, learning_rate, optimizer_type)\n",
    "   \n",
    "        history_train_loss=[]\n",
    "        history_val_loss=[]\n",
    "        history_train_acc=[]\n",
    "        history_val_acc=[]\n",
    "        \n",
    "        for epoch in range(1,500):\n",
    "            \n",
    "                \n",
    "            train_loss,combined_model, optimizer = train_1(list_data_vec_trainB[i],list_data_target_train[i],\n",
    "                                                           combined_model,optimizer)\n",
    "            val_loss                             = validation_1(list_data_vec_valB[i],list_data_target_val[i],\n",
    "                                                                combined_model) \n",
    "        \n",
    "            train_acc                            = test(list_data_vec_trainB[i],list_data_target_train[i],combined_model)\n",
    "            val_acc                              = test(list_data_vec_valB[i],list_data_target_val[i], combined_model)\n",
    "        \n",
    "           \n",
    "            \n",
    "            history_train_loss.append(train_loss.item())\n",
    "            history_val_loss.append(val_loss.item())\n",
    "            history_train_acc.append(train_acc)\n",
    "            history_val_acc.append(val_acc)\n",
    "        \n",
    "            print(f'Fold: {i}')\n",
    "            print(f'Epoch: {epoch:03d}, Train Loss:, {train_loss.item():.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, val Loss:, {val_loss.item():.4f}')\n",
    "            print(f'Epoch: {epoch:03d}, val Acc: {val_acc:.4f}')\n",
    "            print(\" \")\n",
    "        total_history_train_loss.append(history_train_loss)\n",
    "        total_history_val_loss.append(history_val_loss)\n",
    "        total_history_train_acc.append(history_train_acc)\n",
    "        total_history_val_acc.append(history_val_acc)\n",
    "\n",
    "        list_trained_model.append(combined_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ebfee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUNNING THE 10 PRETRAINED MODEL USING INDEPENDENT DATA\n",
    "#======================================================================\n",
    "nCV= 10 # ten crossfold validation \n",
    "list_fold_pred =[]\n",
    "list_fold_targets =[]\n",
    "\n",
    "\n",
    "v_test_loaderB = V_Loader(dataset = final_clean_fingerp_test, batch_size = 1)\n",
    "v_test_target = V_Loader(dataset = total_test_targets, batch_size = 1)\n",
    "\n",
    "for combined_model in list_trained_model:  \n",
    "    list_pred, list_targets = test_1(v_test_loaderB,v_test_target,combined_model)\n",
    "    list_fold_pred.append(list_pred)\n",
    "    list_fold_targets.append(list_targets)\n",
    "    \n",
    "# GET THE PERFORMANCES FROM THE TEST\n",
    "#========================================================================\n",
    "total_performances = performances(list_fold_pred, list_fold_targets, nCV)\n",
    "list_bal_acc = []\n",
    "for sen, spec in zip (total_performances[1] , total_performances[2]):\n",
    "    bal_acc = (sen + spec)/2\n",
    "    list_bal_acc.append(bal_acc)\n",
    "                \n",
    "statistics.mean(list_bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631366d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from prettytable import PrettyTable\n",
    "perf = total_performances\n",
    "model_title = 'Test Perf single model'\n",
    "data_type ='fingerprint vector'\n",
    "Create_Tables(perf, model_title, data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee2e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(list_trained_model):\n",
    "            torch.save(model.state_dict(), str(bal_acc)+\"model_Dense_fingerp\" +str(i)+ \".pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
