{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "260e06f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performances(list_scores, list_targets, nCV):\n",
    "    total_cm=[]\n",
    "    total_acc =[]\n",
    "    total_sen =[]\n",
    "    total_spec =[]\n",
    "    total_MCC =[]\n",
    "    total_AUC =[]\n",
    "    total_ROC =[]\n",
    "    total_PR =[]\n",
    "    total_kappa =[]\n",
    "    total_auroc=[]\n",
    "    total_bal_acc=[]\n",
    "    total_y_pred =[]\n",
    "    total_y_true =[]\n",
    "    \n",
    "    \n",
    "    for j in range(nCV):\n",
    "        y_pred = list_scores[j]\n",
    "        y_true = list_targets[j]\n",
    "        fpr, tpr, thresh_roc = roc_curve(y_true, y_pred, pos_label=1)\n",
    "        prec, rec, thresh_pr = precision_recall_curve(y_true, y_pred, pos_label=1)\n",
    "        total_auroc.append(roc_auc_score(y_true, y_pred))\n",
    "        for i in range(len(y_pred)):\n",
    "           if y_pred[i] <0.5 :\n",
    "               y_pred[i] = 0\n",
    "           else :\n",
    "               y_pred[i] = 1\n",
    "        # save the y_pred in every fold \n",
    "        \n",
    "        total_y_pred.append(y_pred)\n",
    "        total_cm.append(confusion_matrix(y_true, y_pred))\n",
    "        total_MCC.append(matthews_corrcoef(y_true, y_pred))\n",
    "        total_AUC.append(auc(fpr, tpr))\n",
    "        total_ROC.append([fpr,tpr])\n",
    "        total_PR.append([prec,rec])\n",
    "        total_kappa.append(cohen_kappa_score(y_true, y_pred))\n",
    "        \n",
    "    for i in range(nCV):\n",
    "        c_m = total_cm[i]\n",
    "        total_cm_value=sum(sum(c_m))\n",
    "        \n",
    "        Accuracy = (c_m[0,0]+c_m[1,1])/total_cm_value\n",
    "        \n",
    "        Sensitivity = c_m[1,1]/(c_m[1,0]+c_m[1,1])\n",
    "        \n",
    "        Specificity = c_m[0,0]/(c_m[0,0]+c_m[0,1])\n",
    "        \n",
    "        total_acc.append(Accuracy)\n",
    "        total_bal_acc.append((Sensitivity+Specificity)/2)\n",
    "        total_sen.append(Sensitivity)\n",
    "        total_spec.append(Specificity)\n",
    "        \n",
    "    for i in range (nCV): \n",
    "        total_acc[i]         =  round(total_acc[i],4)\n",
    "        total_bal_acc[i]     =  round(total_bal_acc[i],4)\n",
    "        total_sen[i]         =  round(total_sen[i],4)\n",
    "        total_spec[i]        =  round(total_spec[i],4)\n",
    "        total_MCC[i]         =  round(total_MCC[i],4)\n",
    "        total_AUC[i]         =  round(total_AUC[i],4)\n",
    "        total_kappa[i]       =  round(total_kappa[i],4)\n",
    "        total_auroc[i]       =  round(total_auroc[i],4)\n",
    "    total_performances= [total_acc,total_sen,total_spec, total_MCC,total_AUC,total_ROC, total_PR, total_kappa,total_auroc, total_bal_acc]\n",
    "    return total_performances\n",
    "\n",
    "\n",
    "\n",
    "def Get_Mean(perf):\n",
    "    perf_mean=[]\n",
    "    for i in range (5): \n",
    "        perf_mean.append(statistics.mean(perf[i]))\n",
    "    \n",
    "    perf_mean.append(statistics.mean(perf[7]))\n",
    "    perf_mean.append(statistics.mean(perf[8]))\n",
    "    perf_mean.append(statistics.mean(perf[9]))\n",
    "    return  perf_mean # its classified by type of performance \n",
    "\n",
    "def Get_STD(perf):\n",
    "    perf_std=[]\n",
    "    for i in range (5): \n",
    "        perf_std.append(statistics.stdev(perf[i]))\n",
    "    \n",
    "    perf_std.append(statistics.stdev(perf[7]))\n",
    "    perf_std.append(statistics.stdev(perf[8]))\n",
    "    perf_std.append(statistics.stdev(perf[9]))\n",
    "    \n",
    "    return  perf_std # its classified by type of performance \n",
    "\n",
    "\n",
    "\n",
    "# visualization \n",
    "\n",
    "def Create_Tables(perf, model_title, data_type):\n",
    "    \n",
    "        \n",
    "    perf_mean = Get_Mean(perf)\n",
    "    perf_std = Get_STD(perf)\n",
    " \n",
    "    Table_perf_m= PrettyTable([\"Model Name\", \"Data Type\", \"m_ACC\", \"m_SN\", \"m_SP\", \"m_MCC\", \"m_AUC\", \"m_Kappa\",\"m_AUROC\",\"m_Bal_ACC\"])\n",
    "    Table_perf_e= PrettyTable([\"Model Name\", \"Data Type\", \"e_ACC\", \"e_SN\", \"SP\", \"e_MCC\", \"e_AUC\", \"e_Kappa\",\"e_AUROC\",\"e_Bal_ACC\"])\n",
    "    \n",
    "    Table_perf_m.add_row([model_title, data_type, str(round(perf_mean[0],3))\n",
    "                         ,str(round(perf_mean[1],3)),str(round(perf_mean[2],3))\n",
    "                         ,str(round(perf_mean[3],3)),str(round(perf_mean[4],3))\n",
    "                         ,str(round(perf_mean[5],3)),str(round(perf_mean[6],3))\n",
    "                         ,str(round(perf_mean[7],3))])\n",
    "    \n",
    "    Table_perf_e.add_row([model_title, data_type, str(round(perf_std[0],3))\n",
    "                         ,str(round(perf_std[1],3)),str(round(perf_std[2],3))\n",
    "                         ,str(round(perf_std[3],3)),str(round(perf_std[4],3))\n",
    "                         ,str(round(perf_std[5],3)),str(round(perf_std[6],3))\n",
    "                         ,str(round(perf_std[7],3))])\n",
    "    print(Table_perf_m)\n",
    "    print(Table_perf_e)\n",
    "\n",
    "\n",
    "def Plot_Loss_Accuracy(nCV, total_list_train_acc, total_list_train_loss, total_list_val_acc, total_list_val_loss):\n",
    "    \n",
    "    for i in range(nCV):\n",
    "        loss_values = total_list_train_loss[i]\n",
    "        val_loss_values = total_list_val_loss[i]\n",
    "        epochs = range(1, len(loss_values) + 1)\n",
    "        plt.plot(epochs, loss_values, 'b', label='Training loss')\n",
    "        plt.plot(epochs, val_loss_values, 'r', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "   \n",
    "    for i in range(nCV):\n",
    "    \n",
    "        acc_values = total_list_train_acc[i]\n",
    "        val_acc_values = total_list_val_acc[i]\n",
    "        epochs = range(1, len(acc_values) + 1)\n",
    "        plt.plot(epochs, acc_values, 'b', label='Training acc')\n",
    "        plt.plot(epochs, val_acc_values, 'r', label='Validation acc')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "def Plot_Performances(train_perf,test_perf, model_title, data_type):\n",
    "   \n",
    "    perf_train_mean = Get_Mean(train_perf)\n",
    "    perf_train_std = Get_STD(train_perf)\n",
    "\n",
    "    perf_test_mean = Get_Mean(test_perf)\n",
    "    perf_test_std = Get_STD(test_perf)\n",
    "    \n",
    "    \n",
    "    type_perf = ['ACC', 'SN', 'SP', 'MCC', 'AUC']\n",
    "    x_indexes = np.arange(len(perf_train_mean)) #it should be 5 \n",
    "    width = 0.3\n",
    "    \n",
    "      \n",
    "    fig, axs = plt.subplots(1, 1,figsize=(7,7))\n",
    "    fig.suptitle('Performances '+ model_title +\" \"+ data_type )\n",
    "    #axs[0].figure(figsize=(7,5))\n",
    "    #axs[1].figure(figsize=(7,5))\n",
    "    \n",
    "    axs.set_title( model_title)\n",
    "    \n",
    "    \n",
    "    axs.bar(x_indexes-width/2, perf_train_mean,yerr =perf_train_std,width=width, color =['blue'], label = \"train\")\n",
    "    \n",
    "    axs.bar(x_indexes+width/2, perf_test_mean,yerr =perf_test_std,width=width, color =['green'], label =\"test\" )\n",
    "    \n",
    "    axs.legend(loc='lower right')\n",
    "    axs.set(xlabel='perf types',ylabel ='perf in %')\n",
    "    \n",
    "    #axs[0].xticks(ticks =x_indexes, labels =type_perf)\n",
    "    \n",
    "    \n",
    "   \n",
    "    plt.setp(axs, xticks=x_indexes, xticklabels=type_perf)\n",
    "    \n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c9541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
